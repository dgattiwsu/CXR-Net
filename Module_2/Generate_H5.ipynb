{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate JMS or V7 H5 files for CXR images and masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "\n",
    "import os, sys, shutil\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "import csv, h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "from MODULES.Generators import get_generator, DataGenerator\n",
    "from MODULES.Losses import other_metrics_binary_class\n",
    "from MODULES.Constants import _Params, _Paths\n",
    "from MODULES.Utils import get_class_threshold, standardize, commonelem_set\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import model_from_json \n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from tensorflow.compat.v1.logging import INFO, set_verbosity\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "# ### MODEL AND RUN SELECTION\n",
    "HEIGHT, WIDTH, CHANNELS, IMG_COLOR_MODE, MSK_COLOR_MODE, NUM_CLASS, \\\n",
    "    KS1, KS2, KS3, DL1, DL2, DL3, NF, NFL, NR1, NR2, DIL_MODE, W_MODE, LS, \\\n",
    "    SHIFT_LIMIT, SCALE_LIMIT, ROTATE_LIMIT, ASPECT_LIMIT, U_AUG, \\\n",
    "    TRAIN_SIZE, VAL_SIZE, DR1, DR2, CLASSES, IMG_CLASS, MSK_FLOAT, MSK_THRESHOLD, \\\n",
    "    MRA, MRALEVEL, MRACHANNELS, WAVELET, WAVEMODE, WST, WST_J, WST_L, WST_FIRST_IMG, \\\n",
    "    SCALE_BY_INPUT, SCALE_THRESHOLD = _Params()\n",
    "    \n",
    "TRAIN_IMG_PATH, TRAIN_MSK_PATH, TRAIN_MSK_CLASS, VAL_IMG_PATH, \\\n",
    "        VAL_MSK_PATH, VAL_MSK_CLASS = _Paths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263 1532\n"
     ]
    }
   ],
   "source": [
    "# In[4]:\n",
    "\n",
    "# HFHS set\n",
    "train_df = pd.read_csv(\"selected_train_020521_index.csv\",index_col = 0)\n",
    "valid_df = pd.read_csv(\"selected_valid_020521_index.csv\",index_col = 0)\n",
    "\n",
    "n_train = len(train_df)\n",
    "n_valid = len(valid_df)\n",
    "# n_test = len(test_df)\n",
    "print(n_train,n_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "# ### DATASET PREPARATION FOR STANDARDIZED IMAGES as H5 files\n",
    "\n",
    "# Source directories for images and masks\n",
    "# HFHS set\n",
    "IMAGE_DIR = \"dataset/selected_COVID_pos4_neg5_image_resized_equalized/\"\n",
    "\n",
    "# For JMS database\n",
    "# MASK_DIR = \"dataset/selected_COVID_pos4_neg5_masks_float_952_3/\"\n",
    "# For V7 database\n",
    "MASK_DIR = \"dataset/selected_COVID_pos4_neg5_masks_float_6395_1/\"\n",
    "\n",
    "# Target directory for H5 file containing both images and masks\n",
    "# For JMS database\n",
    "# H5_IMAGE_DIR = \"dataset/COVID_standardized_pos4_neg5_image_expand_float_952_3_threshold_H5_CLASSWEIGHT/\"\n",
    "# For V7 database\n",
    "H5_IMAGE_DIR = \"dataset/COVID_standardized_pos4_neg5_image_expand_float_6395_1_threshold_H5_CLASSWEIGHT/\"\n",
    "\n",
    "print(IMAGE_DIR)\n",
    "print(H5_IMAGE_DIR)\n",
    "\n",
    "pwd = os.getcwd()\n",
    "if not os.path.isdir(H5_IMAGE_DIR):\n",
    "    os.mkdir(H5_IMAGE_DIR)\n",
    "    \n",
    "# Arrays for training and validation set\n",
    "\n",
    "# Training set\n",
    "train_image_mat = np.zeros((HEIGHT,WIDTH,n_train,1))\n",
    "train_mask_mat = np.zeros((HEIGHT,WIDTH,n_train,1))\n",
    "train_label_mat = np.zeros((n_train,2))\n",
    "train_weight_mat = np.zeros((n_train,1))\n",
    "train_df_index = train_df.index.tolist()\n",
    "\n",
    "# Validation set \n",
    "valid_df_index = valid_df.index.tolist()\n",
    "\n",
    "# Loop over the training set and calculate mean and std\n",
    "for i in range(n_train):\n",
    "    print(f'{i},index={train_df.index[i]}')\n",
    "    train_image_name, train_pos_label, train_neg_label, train_weight = \\\n",
    "    train_df.iloc[i]['Image'],\\\n",
    "    train_df.iloc[i]['Positive'],\\\n",
    "    train_df.iloc[i]['Negative'],\\\n",
    "    train_df.iloc[i]['ClassWeight']\n",
    "\n",
    "    train_image = cv2.imread(IMAGE_DIR + train_image_name, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Resize and equalize if this was not already done during datasets preparation\n",
    "    # train_image = cv2.resize(train_image, (WIDTH, HEIGHT), cv2.INTER_LINEAR)\n",
    "    # train_image = cv2.equalizeHist(train_image)\n",
    "\n",
    "    train_image = np.expand_dims(train_image,axis=-1)\n",
    "    \n",
    "    train_image_mat[:,:,i] = train_image\n",
    "    train_label_mat[i,:] = [train_pos_label,train_neg_label]\n",
    "    train_weight_mat[i,:] = train_weight\n",
    "    \n",
    "    # External learned mask of segmented lungs\n",
    "    train_learned_mask = cv2.imread(MASK_DIR + train_image_name, cv2.IMREAD_GRAYSCALE).astype('float64')\n",
    "    train_learned_mask /= 255\n",
    "    train_learned_mask = np.expand_dims(train_learned_mask,axis=-1)\n",
    "    \n",
    "    # Internal thresholded mask to eliminate dark regions of the CXR \n",
    "    low_ind = train_image < 6\n",
    "    high_ind = train_image > 225    \n",
    "    train_thresholded_mask = np.ones_like(train_image)\n",
    "    train_thresholded_mask[low_ind] = 0\n",
    "    train_thresholded_mask[high_ind] = 0\n",
    "        \n",
    "    # Combine the two masks\n",
    "    train_mask_mat[:,:,i] = np.multiply(train_thresholded_mask,train_learned_mask)\n",
    "    \n",
    "# IMPORTANT: For best result we standardize the entire set.     \n",
    "train_image_mat, train_image_mean, train_image_std = standardize(train_image_mat, by_layer=False)\n",
    "\n",
    "for i in range(n_train):    \n",
    "    train_image_name = train_df.iloc[i]['Image']\n",
    "\n",
    "    with h5py.File(H5_IMAGE_DIR + train_image_name[:-4] + '.h5', 'w') as hf: \n",
    "        # Images\n",
    "        Xset = hf.create_dataset(\n",
    "            name='X',\n",
    "            data=np.squeeze(train_image_mat[:,:,i,:]),\n",
    "            shape=(HEIGHT, WIDTH, 1),\n",
    "            maxshape=(HEIGHT, WIDTH, 1),\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=9)\n",
    "        \n",
    "        # Masks\n",
    "        Mset = hf.create_dataset(\n",
    "            name='M',\n",
    "            data=np.squeeze(train_mask_mat[:,:,i,:]),\n",
    "            shape=(HEIGHT, WIDTH, 1),\n",
    "            maxshape=(HEIGHT, WIDTH, 1),\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=9)\n",
    "\n",
    "        # Labels\n",
    "        yset = hf.create_dataset(\n",
    "            name='y',\n",
    "            data=np.squeeze(train_label_mat[i,:]))\n",
    "        \n",
    "        # Class weights\n",
    "        wset = hf.create_dataset(\n",
    "            name='w',\n",
    "            data=np.squeeze(train_weight_mat[i,:]))   \n",
    "        \n",
    "\n",
    "# Loop over the validation set\n",
    "\n",
    "for i in range(n_valid):\n",
    "    print(f'{i},index={valid_df.index[i]}')\n",
    "    valid_image_name, valid_pos_label, valid_neg_label, valid_weight = \\\n",
    "    valid_df.iloc[i]['Image'],\\\n",
    "    valid_df.iloc[i]['Positive'],\\\n",
    "    valid_df.iloc[i]['Negative'],\\\n",
    "    valid_df.iloc[i]['ClassWeight']\n",
    "\n",
    "    valid_image = cv2.imread(IMAGE_DIR + valid_image_name, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "# Resize or equalize if this was not already done during datasets preparation    \n",
    "    # valid_image = cv2.resize(valid_image, (WIDTH, HEIGHT), cv2.INTER_LINEAR)\n",
    "    # valid_image = cv2.equalizeHist(valid_image)\n",
    "\n",
    "    valid_image = np.expand_dims(valid_image,axis=-1)\n",
    "        \n",
    "    # External learned mask of segmented lungs\n",
    "    valid_learned_mask = cv2.imread(MASK_DIR + valid_image_name, cv2.IMREAD_GRAYSCALE).astype('float64')\n",
    "    valid_learned_mask /= 255\n",
    "    valid_learned_mask = np.expand_dims(valid_learned_mask,axis=-1)\n",
    "    \n",
    "    # Internal thresholded mask    \n",
    "    low_ind = valid_image < 6\n",
    "    high_ind = valid_image > 225    \n",
    "    valid_thresholded_mask = np.ones_like(valid_image)\n",
    "    valid_thresholded_mask[low_ind] = 0\n",
    "    valid_thresholded_mask[high_ind] = 0\n",
    "\n",
    "    # Combine the two masks\n",
    "    valid_mask = np.multiply(valid_thresholded_mask,valid_learned_mask)\n",
    "    \n",
    "    # Standardization with training mean and std \n",
    "    valid_image = valid_image.astype(np.float64)\n",
    "    valid_image -= train_image_mean\n",
    "    valid_image /= train_image_std        \n",
    "    \n",
    "    with h5py.File(H5_IMAGE_DIR + valid_image_name[:-4] + '.h5', 'w') as hf: \n",
    "        # Images\n",
    "        Xset = hf.create_dataset(\n",
    "            name='X',\n",
    "            data=valid_image,\n",
    "            shape=(HEIGHT, WIDTH, 1),\n",
    "            maxshape=(HEIGHT, WIDTH, 1),\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=9)\n",
    "        \n",
    "        # Masks\n",
    "        Mset = hf.create_dataset(\n",
    "            name='M',\n",
    "            data=valid_mask,\n",
    "            shape=(HEIGHT, WIDTH, 1),\n",
    "            maxshape=(HEIGHT, WIDTH, 1),\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=9)\n",
    "        \n",
    "        # Labels\n",
    "        yset = hf.create_dataset(\n",
    "            name='y',\n",
    "            data=[valid_pos_label,valid_neg_label])\n",
    "        \n",
    "        # Class weights\n",
    "        wset = hf.create_dataset(\n",
    "            name='w',\n",
    "            data=valid_weight) \n",
    "        \n",
    "        \n",
    "# Generate json dictionary with standardization parameters\n",
    "h5_dict = {\"mean\":train_image_mean,\"std\":train_image_std}          \n",
    "\n",
    "# For JMS database\n",
    "# with open(H5_IMAGE_DIR + 'standardization_parameters_JMS.json', 'w') as filehandle:\n",
    "#     json.dump(h5_dict, filehandle)\n",
    "    \n",
    "# For V7 database    \n",
    "with open(H5_IMAGE_DIR + 'standardization_parameters_V7.json', 'w') as filehandle:\n",
    "    json.dump(h5_dict, filehandle)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate json dictionary (do not regenerate if number of elements was changed by duplicating some)\n",
    "\n",
    "train_h5_name_list = []\n",
    "valid_h5_name_list = []\n",
    "\n",
    "for i in range(n_train):\n",
    "    # print(f'{i},index={valid_df.index[i]}')\n",
    "    train_image_name = train_df.iloc[i]['Image']\n",
    "    train_h5_name_list.append(train_image_name[:-4] + '.h5') \n",
    "for i in range(n_valid):\n",
    "    # print(f'{i},index={valid_df.index[i]}')\n",
    "    valid_image_name = valid_df.iloc[i]['Image']\n",
    "    valid_h5_name_list.append(valid_image_name[:-4] + '.h5') \n",
    "\n",
    "h5_dict = {\"train\":train_h5_name_list,\"valid\":valid_h5_name_list}\n",
    "\n",
    "# HFHS set\n",
    "with open(H5_IMAGE_DIR + 'pos4_neg5_datasets_CLASSWEIGHT_only.json', 'w') as filehandle:\n",
    "    json.dump(h5_dict, filehandle)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check H5 file generation\n",
    "\n",
    "print(IMAGE_DIR)\n",
    "print(H5_IMAGE_DIR)\n",
    "\n",
    "h5_files = [f for f in listdir(H5_IMAGE_DIR) if isfile(join(H5_IMAGE_DIR, f))]\n",
    "h5_files.sort()\n",
    "\n",
    "# Select one image\n",
    "with h5py.File(H5_IMAGE_DIR + h5_files[9], 'r') as f:\n",
    "    X_h5 = np.array(f.get(\"X\"))\n",
    "    M_h5 = np.array(f.get(\"M\"))\n",
    "    y_h5 = np.array(f.get(\"y\"))\n",
    "    w_h5 = np.array(f.get(\"w\"))\n",
    "    \n",
    "fig = plt.figure(figsize=(20,10))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.imshow(np.squeeze(X_h5), cmap=\"gray\")\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.imshow(np.squeeze(M_h5), cmap=\"gray\")       \n",
    "\n",
    "# plt.savefig(working_img_mask_path + name + '_img_and_pred_mask.png') \n",
    "# plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_files = [f for f in listdir(H5_IMAGE_DIR) if isfile(join(H5_IMAGE_DIR, f))]\n",
    "h5_files.sort()\n",
    "# print(h5_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no common elements between these two sets.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate slice number between training and validation or test set\n",
    "\n",
    "# HFHS set\n",
    "with open(H5_IMAGE_DIR + \"pos4_neg5_datasets_CLASSWEIGHT_only.json\") as json_file:\n",
    "    dataset = json.load(json_file)\n",
    "    \n",
    "train_len = len(dataset['train'])\n",
    "train_id_list = [] \n",
    "for name in dataset['train']:\n",
    "    train_id_list.append(name)\n",
    "    \n",
    "valid_len = len(dataset['valid'])\n",
    "valid_id_list = [] \n",
    "for name in dataset['valid']:\n",
    "    valid_id_list.append(name)\n",
    "    \n",
    "# test_len = len(dataset['test'])\n",
    "# test_id_list = [] \n",
    "# for name in dataset['test']:\n",
    "#     test_id_list.append(name)\n",
    "\n",
    "# Checking common elements in two lists\n",
    "common_train_valid_slices = commonelem_set(train_id_list, valid_id_list)\n",
    "print(f'{common_train_valid_slices}')\n",
    "# common_train_test_slices = commonelem_set(train_id_list, test_id_list)\n",
    "# print(f'Common slices between train and test sets: {common_train_test_slices}')\n",
    "# common_valid_test_slices = commonelem_set(valid_id_list, test_id_list)\n",
    "# print(f'Common slices between valid and test sets: {common_valid_test_slices}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
